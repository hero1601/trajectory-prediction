{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":97693,"databundleVersionId":11656558,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CSE251B Project Milestone Starter File","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Import Dependencies:","metadata":{}},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch_geometric.data import Data, Batch\nimport tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Load the Dataset","metadata":{}},{"cell_type":"markdown","source":"#### You need to describe in your own words what the dataset is about, and use mathematical language and formulate your prediction task on the submitted PDF file for Question 1 Problem A.","metadata":{}},{"cell_type":"markdown","source":"#### Here we are loading the dataset from the local directory. And answer Question 1 Problem B","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_npz = np.load('/kaggle/input/cse-251-b-2025/train.npz')\ntrain_data = train_npz['data']\ntest_npz  = np.load('/kaggle/input/cse-251-b-2025/test_input.npz')\ntest_data  = test_npz['data']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_data.shape, test_data.shape)\n\n# Split once for later use\nX_train = train_data[..., :50, :]\nY_train = train_data[:, 0, 50:, :2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_heatmap(data, title=None, bins=5):\n    plt.figure(figsize=(6, 6))\n\n    x_max = data[..., 0].max()\n    x_min = data[..., 0].min()\n    y_max = data[..., 1].max()\n    y_min = data[..., 1].min()\n\n    plt.hist2d(data[:, 0], data[:, 1], bins=bins, cmap='hot')\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.title(title)\n    plt.colorbar(label='Density')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xy_in = train_data[:, :, :50, :2].reshape(-1, 2)\n# only find the x, y != 0\nxy_in_not_0 = xy_in[(xy_in[:, 0] != 0) & (xy_in[:, 1] != 0)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_heatmap(xy_in, title='Heatmap of XY In', bins=5)\nplot_heatmap(xy_in_not_0, title='Heatmap of XY In (non-zero)', bins=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_heatmap(xy_in, title='Heatmap of XY In', bins=50)\nplot_heatmap(xy_in_not_0, title='Heatmap of XY In (non-zero)', bins=50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Try to play around with dataset for training and testing, make exploratory analysis on the dataset for bonus points(up to 2)","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Setting up the Training and Testing","metadata":{}},{"cell_type":"markdown","source":"### Example Code:","metadata":{}},{"cell_type":"code","source":"class TrajectoryDatasetTrain(Dataset):\n    def __init__(self, data, scale=10.0, augment=True):\n        \"\"\"\n        data: Shape (N, 50, 110, 6) Training data\n        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n        augment: Whether to apply data augmentation (only for training)\n        \"\"\"\n        self.data = data\n        self.scale = scale\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        scene = self.data[idx]\n        # Getting 50 historical timestamps and 60 future timestamps\n        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (60, 2)\n        \n        # Data augmentation(only for training)\n        if self.augment:\n            if np.random.rand() < 0.5:\n                theta = np.random.uniform(-np.pi, np.pi)\n                R = np.array([[np.cos(theta), -np.sin(theta)],\n                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n                # Rotate the historical trajectory and future trajectory\n                hist[..., :2] = hist[..., :2] @ R\n                hist[..., 2:4] = hist[..., 2:4] @ R\n                future = future @ R\n            if np.random.rand() < 0.5:\n                hist[..., 0] *= -1\n                hist[..., 2] *= -1\n                future[:, 0] *= -1\n\n        # Use the last timeframe of the historical trajectory as the origin\n        origin = hist[0, 49, :2].copy()  # (2,)\n        hist[..., :2] = hist[..., :2] - origin\n        future = future - origin\n\n        # Normalize the historical trajectory and future trajectory\n        hist[..., :4] = hist[..., :4] / self.scale\n        future = future / self.scale\n\n        data_item = Data(\n            x=torch.tensor(hist, dtype=torch.float32),\n            y=future.type(torch.float32),\n            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n            scale=torch.tensor(self.scale, dtype=torch.float32),\n        )\n\n        return data_item\n    \n\nclass TrajectoryDatasetTest(Dataset):\n    def __init__(self, data, scale=10.0):\n        \"\"\"\n        data: Shape (N, 50, 110, 6) Testing data\n        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n        \"\"\"\n        self.data = data\n        self.scale = scale\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Testing data only contains historical trajectory\n        scene = self.data[idx]  # (50, 50, 6)\n        hist = scene.copy()\n        \n        origin = hist[0, 49, :2].copy()\n        hist[..., :2] = hist[..., :2] - origin\n        hist[..., :4] = hist[..., :4] / self.scale\n\n        data_item = Data(\n            x=torch.tensor(hist, dtype=torch.float32),\n            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n            scale=torch.tensor(self.scale, dtype=torch.float32),\n        )\n        return data_item","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Answer Question related to Your Computational Platform and GPU for Question 2 Problem A","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(251)\nnp.random.seed(42)\n\nscale = 7.0\n\nN = len(train_data)\nval_size = int(0.2 * N)\ntrain_size = N - val_size\nprint(N)\n\ntrain_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\nval_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n\n# Set device for training speedup\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\n    print(\"Using Apple Silicon GPU\")\nelif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"Using CUDA GPU\")\nelse:\n    device = torch.device('cpu')\n    print('cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Your Model for Question 2 Problem B (Include your model architecture pictures and also can use some mathmatical equations to explain your model in your report)","metadata":{}},{"cell_type":"code","source":"class LinearRegressionModel(nn.Module):\n    def __init__(self, input_dim=50 * 50 * 2, output_dim=60 * 2):\n        super(LinearRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, data):\n        x = data.x[..., :2] # (batch*50, 50, 2)\n        x = x.reshape(-1, 50 * 50 * 2) # (batch, 5000)\n        x = self.linear(x)\n        return x.view(-1, 60, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, input_features, output_features):\n        super(MLP, self).__init__()\n        \n        # Define the layers\n        self.flatten = nn.Flatten()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_features, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(256, output_features)\n        )\n    \n    def forward(self, data):\n        x = data.x\n        x = x[:, :, :, :2] # (batch, 50, 50, 2)\n        x = x.reshape(-1, 50 * 50 * 6)\n        x = self.mlp(x)\n        return x.view(-1, 60, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### This Model will be covered during Week 6 Lecture (If you don't understand it for now, don't worry, we will cover it in the lecture, or you can ask in the office hours)","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_dim=6, hidden_dim=200, output_dim=60 * 2):\n        super(LSTM, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x = data.x\n        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n        x = x[:, 0, :, :] # Only Consider ego agent index 0\n\n        lstm_out, _ = self.lstm(x)\n        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n        out = self.fc(lstm_out[:, -1, :])\n        return out.view(-1, 60, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LSTMWithAttention(nn.Module):\n    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60*2):\n        super(LSTMWithAttention, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.attn = nn.Linear(hidden_dim, 1)  # attention scores\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x = data.x.reshape(-1, 50, 50, 6)\n        x = x[:, 0, :, :]\n\n        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n\n        # Attention: compute weights\n        attn_scores = self.attn(lstm_out)  # (batch_size, seq_len, 1)\n        attn_weights = nn.functional.softmax(attn_scores, dim=1)  # (batch_size, seq_len, 1)\n\n        # Weighted sum of LSTM outputs\n        context = (attn_weights * lstm_out).sum(dim=1)\n\n        # Final output projection\n        out = self.fc(context)  # (batch_size, 60*2)\n        return out.view(-1, 60, 2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MeanPoolSocialLSTM(nn.Module):\n    def __init__(self, input_dim=6, hidden_dim=128, output_dim=60 * 2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n\n    def forward(self, data):\n        x = data.x.view(-1, 50, 50, 6)  # (B, N, T, D)\n        B, N, T, D = x.shape\n        x_flat = x.view(B * N, T, D)\n\n        lstm_out, (h_n, _) = self.lstm(x_flat)\n        h_n = h_n[0].view(B, N, -1)  # (B, N, H)\n\n        ego_feat = h_n[:, 0, :]                # (B, H)\n        pooled_feat = h_n.mean(dim=1)          # (B, H)\n\n        feat = torch.cat([ego_feat, pooled_feat], dim=-1)  # (B, 2H)\n        out = self.fc(feat)\n        return out.view(B, 60, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GRU(nn.Module):\n    def __init__(self, input_dim=6, hidden_dim=120, output_dim=60 * 2):\n        super(GRU, self).__init__()\n        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, data):\n        x = data.x\n        x= x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)\n        x = x[:, 0, :, :] # Only Consider ego agent index 0\n\n        lstm_out, _ = self.gru(x)\n        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output\n        out = self.fc(lstm_out[:, -1, :])\n        return out.view(-1, 60, 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AttentionSocialLSTM(nn.Module):\n    def __init__(self, input_dim=8, hidden_dim=128, output_dim=60 * 2):\n        super(AttentionSocialLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # ego + attention pooled\n\n    def forward(self, data):\n        x = data.x.view(-1, 50, 50, 6)  # (B, N, T, 6)\n        B, N, T, D = x.shape\n\n        # Compute speed\n        vel_x = x[..., 2]\n        vel_y = x[..., 3]\n        speed = torch.sqrt(vel_x**2 + vel_y**2 + 1e-6).unsqueeze(-1)  # (B, N, T, 1)\n\n        # Compute acceleration\n        accel = speed[:, :, 1:] - speed[:, :, :-1]\n        accel = F.pad(accel, (0, 0, 1, 0), mode='constant', value=0)  # (B, N, T, 1)\n\n        # Concatenate to original features\n        x = torch.cat([x, speed, accel], dim=-1)  # (B, N, T, 8)\n        x_flat = x.view(B * N, T, -1)             # (B*N, T, 8)\n\n        # LSTM encoding\n        lstm_out, (h_n, _) = self.lstm(x_flat)\n        h_n = h_n[0].view(B, N, -1)               # (B, N, hidden_dim)\n\n        ego_feat = h_n[:, 0, :].unsqueeze(1)      # (B, 1, hidden_dim)\n        all_feat = h_n                            # (B, N, hidden_dim)\n\n        # Create agent mask\n        agent_mask = (x.abs().sum(dim=-1).sum(dim=-1) != 0).float()  # (B, N)\n\n        # Attention\n        attn_scores = torch.bmm(ego_feat, all_feat.transpose(1, 2))  # (B, 1, N)\n        attn_scores = attn_scores.masked_fill(agent_mask.unsqueeze(1) == 0, -1e9)\n        attn_weights = F.softmax(attn_scores, dim=-1)                # (B, 1, N)\n        attn_feat = torch.bmm(attn_weights, all_feat).squeeze(1)    # (B, hidden_dim)\n\n        # Final prediction\n        ego_feat = ego_feat.squeeze(1)\n        combined_feat = torch.cat([ego_feat, attn_feat], dim=-1)     # (B, 2*hidden_dim)\n        out = self.fc(combined_feat)                                 # (B, 120)\n        return out.view(B, 60, 2)                                     # (B, 60, ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Your Optimizer and Hyperparameters for Question 2 Problem A (Try to use different optimizers and hyperparameters for your model and see how it affects the performance of your model)","metadata":{}},{"cell_type":"code","source":"# model = LinearRegressionModel().to(device)\n# model = MLP(50 * 50 * 6, 60 * 2).to(device)\n# model = LSTM()\n# model = GRU()\n# model  = TransformerModel()\n# model = LSTMWithAttention()\nmodel = AttentionSocialLSTM()\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\nmodel = model.to(device)\n\n# optimizer = optim.Adam(model.parameters(), lr=1e-3)\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.25) # You can try different schedulers\nearly_stopping_patience = 10\nbest_val_loss = float('inf')\nno_improvement = 0\ncriterion = nn.MSELoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Using the Simple Linear Regression Model for Question 2B and Visualize the validation loss(MAE) (Hint: You should adapt the code for training loss and try to draw graphs as specified in the project description)","metadata":{}},{"cell_type":"code","source":"for epoch in tqdm.tqdm(range(100), desc=\"Epoch\", unit=\"epoch\"):\n    # ---- Training ----\n    model.train()\n    train_loss = 0\n    for batch in train_dataloader:\n        batch = batch.to(device)\n        pred = model(batch)\n        y = batch.y.view(batch.num_graphs, 60, 2)\n        loss = criterion(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_loss += loss.item()\n    \n    # ---- Validation ----\n    model.eval()\n    val_loss = 0\n    val_mae = 0\n    val_mse = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            batch = batch.to(device)\n            pred = model(batch)\n            y = batch.y.view(batch.num_graphs, 60, 2)\n            val_loss += criterion(pred, y).item()\n\n            # show MAE and MSE with unnormalized data\n            pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n            y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n            val_mae += nn.L1Loss()(pred, y).item()\n            val_mse += nn.MSELoss()(pred, y).item()\n    \n    train_loss /= len(train_dataloader)\n    val_loss /= len(val_dataloader)\n    val_mae /= len(val_dataloader)\n    val_mse /= len(val_dataloader)\n    scheduler.step()\n    # scheduler.step(val_loss)\n    \n    tqdm.tqdm.write(f\"Epoch {epoch:03d} | Learning rate {optimizer.param_groups[0]['lr']:.6f} | train normalized MSE {train_loss:8.4f} | val normalized MSE {val_loss:8.4f}, | val MAE {val_mae:8.4f} | val MSE {val_mse:8.4f}\")\n    if val_loss < best_val_loss - 1e-3:\n        best_val_loss = val_loss\n        no_improvement = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n    else:\n        no_improvement += 1\n        if no_improvement >= early_stopping_patience:\n            print(\"Early stop!\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Randomly sample validation dataset and Visualize the ground truth and your predictions on a 2D plane for Question 3 Problem A","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\ndef plot_trajectory(ax, pred, gt, title=None):\n    ax.cla()\n    # Plot the predicted future trajectory\n    ax.plot(pred[0,:60,0], pred[0,:60,1], color='red', label='Predicted Future Trajectory')\n    \n    # Plot the ground truth future trajectory\n    ax.plot(gt[0,:60,0], gt[0,:60,1], color='blue', label='Ground Truth Future Trajectory')\n    \n    # Optionally set axis limits, labels, and title.\n    x_max = max(pred[..., 0].max(), gt[..., 0].max())\n    x_min = min(pred[..., 0].min(), gt[..., 0].min())\n    y_max = max(pred[..., 1].max(), gt[..., 1].max())\n    y_min = min(pred[..., 1].min(), gt[..., 1].min())\n    \n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    \n    if title:\n        ax.set_title(title)\n    \n    ax.legend()\n    ax.grid(True, linestyle='--', alpha=0.7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\n# randomly select 4 samples from the validation set\nrandom_indices = random.sample(range(len(val_dataset)), 4)\nfig, axes = plt.subplots(2, 2, figsize=(20, 10))\naxes = axes.flatten()  # Flatten the array to iterate single axes objects\n\nfor i, idx in enumerate(random_indices):\n    batch = val_dataset[idx]\n    batch = batch.to(device)\n    pred = model(batch)\n    gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0)\n\n    pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n    gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0) * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n\n    pred = pred.detach().cpu().numpy()\n    gt = gt.detach().cpu().numpy()\n\n    # Plot the trajectory using the i-th axis\n    plot_trajectory(axes[i], pred, gt, title=f\"Sample {idx}\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Output your predictions of the best model on the test set","metadata":{}},{"cell_type":"code","source":"test_dataset = TrajectoryDatasetTest(test_data, scale=scale)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,\n                         collate_fn=lambda xs: Batch.from_data_list(xs))\n\nbest_model = torch.load(\"best_model.pt\")\n# model = LinearRegressionModel().to(device)\n# model = MLP(50 * 50 * 6, 60 * 2).to(device)\n# model = LSTM().to(device)\n\nmodel.load_state_dict(best_model)\nmodel.eval()\n\npred_list = []\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        pred_norm = model(batch)\n        \n        # Reshape the prediction to (N, 60, 2)\n        pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n        pred_list.append(pred.cpu().numpy())\npred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\npred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\noutput_df = pd.DataFrame(pred_output, columns=['x', 'y'])\noutput_df.index.name = 'index'\noutput_df.to_csv('submission.csv', index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Summarize your experiments and results in table and figures in the submitted PDF file for Question 3 Problem A","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Analyze the results, identify the issues and plan for the improvement in the submitted PDF file for Question 3 Problem B","metadata":{}}]}